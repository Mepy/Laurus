///|
pub(all) struct Grammar {
  rules : Array[Array[Sentence]] // rules[sort][rule_i]
} derive(Show)

///|
/// grammar[sort][rule_i] = sort -> sentence | lookahead,
/// cur points to the first symbol
struct Item {
  sort : Sort
  rule_i : RuleIndex
  sentence : Sentence
  cur : Index
  lookahead : Token
} derive(Show, Hash, Eq, Compare)

///|
struct ItemSet(Set[Item]) derive(Show, Eq)

///|
impl Hash for ItemSet with hash_combine(self : ItemSet, hasher : Hasher) {
  hasher.combine(self.hash())
}

///|
impl Hash for ItemSet with hash(self : ItemSet) -> Int {
  let s = self.0.to_array()
  s.sort()
  s.hash()
}

///|
struct State {
  items : ItemSet
} derive(Show, Hash, Eq)

///|
pub(all) struct LR1Maker {
  grammar : Grammar
  sort_len : Symbol // sort < sort_len <= token, we reserve sort_len as the first token, the EOF
  empty : FixedArray[Bool]
  supergraph : SuperGraph
  first : FixedArray[Set[Token]]
  worklist : Array[(State, StateIndex)] // state and its index
  state_indices : Map[State, StateIndex]
  mut states : Array[Set[Item]]

  // goto_table[state_i][sort] = state_j
  goto_table : Array[Map[Sort, StateIndex]]
  // shift_table[state_i][token] = (state_j, [(sort1, rule_i1), (sort2, rule_i2), ...])
  shift_table : Array[Map[Token, (StateIndex, Array[(Sort, RuleIndex)])]]
  // reduce_table[state_i][token] = [(sort1, rule_i1), (sort2, rule_i2), ...]
  reduce_table : Array[Map[Token, Array[(Sort, RuleIndex)]]]
  // first_table[state_i] = {f1, f2, ...} first_set of the state
  first_table : Array[Set[Token]]
} derive(Show)

///|
pub(all) struct LR1 {
  grammar : Grammar
  sort_len : Symbol
  states : Array[Set[Item]]

  // LR1::goto_table[sort][state_i] 
  // == state_j ==
  // LR1Maker::goto_table[state_i][sort]
  goto_table : Array[Map[StateIndex, StateIndex]]
  // shift_table[state_i][token] = (state_j, dfa_j)
  // where dfa_j is the index of dfa at state_j
  shift_table : Array[Map[Token, (StateIndex, Index)]]
  reduce_table : Array[Map[Token, (Sort, RuleIndex, RuleSize)]]
  // dfa_tokens[i] -> tokens of the dfa[i] will returned, 
  // and tokens here begin with 0 instead of sort_len in LR1Maker
  // generated by LR1Maker::first_table
  dfa_tokens : Array[Array[Token]]
} derive(Show)

///|
pub fn LR1Maker::new(grammar : Grammar) -> LR1Maker {
  let sort_len = grammar.rules.length()
  let empty = LR1Maker::initialize_empty(grammar, sort_len)
  let (supergraph, first) = LR1Maker::initialize_first(grammar, sort_len, empty)
  let maker = LR1Maker::{
    grammar,
    sort_len,
    empty,
    supergraph,
    first,
    worklist: Array::new(),
    state_indices: Map::new(),
    states: Array::new(),
    goto_table: Array::new(),
    shift_table: Array::new(),
    reduce_table: Array::new(),
    first_table: Array::new(),
  }
  maker
}

///|
#inline
fn LR1Maker::is_token(self : Self, sy : Symbol) -> Bool {
  self.sort_len <= sy
}

///|
fn LR1Maker::initialize_empty(
  grammar : Grammar,
  sort_len : Symbol,
) -> FixedArray[Bool] {
  let empty : FixedArray[Bool] = FixedArray::make(sort_len, false)
  let rules_for_empty : FixedArray[Array[Sentence]] = FixedArray::makei(
    sort_len,
    i => Array::new(capacity=grammar.rules[i].length()),
  )
  let mut not_fix = false
  for sort in 0..<sort_len {
    let rules : Array[Sentence] = grammar.rules[sort]
    let rules_for_empty : Array[Sentence] = rules_for_empty[sort] // store those containing only sorts
    for sentence in rules {
      if sentence.length() == 0 {
        empty[sort] = true // empty introduced
        not_fix = true // indicates that we need to update empty
      } else if empty[sort] == false { // only filling rules_for_empty when need to update
        if sentence.iter().all(symbol => symbol < sort_len) { // all sorts
          rules_for_empty.push(sentence)
        }
      }
    }
  }
  let workset : Set[Sort] = Set::new(capacity=sort_len)
  for sort in 0..<sort_len {
    if empty[sort] == false {
      workset.add(sort)
    }
  }
  while not_fix { // update empty
    not_fix = false
    for sort in workset {
      let rules : Array[Sentence] = rules_for_empty[sort]
      for sentence in rules {
        // sentence.length() > 0, asserted by above code
        // s -> s1 s2 s3 ... sn
        if sentence.iter().all(s => empty[s]) { // update empty[]
          empty[sort] = true
          workset.remove(sort)
          not_fix = true
          break // It's OK to skip following sentences of sort
        }
      }
    }
  }
  empty
}

///|
fn LR1Maker::initialize_first(
  grammar : Grammar,
  sort_len : Symbol,
  empty : FixedArray[Bool],
) -> (SuperGraph, FixedArray[Set[Token]]) {
  let first : FixedArray[Set[Token]] = FixedArray::makei(sort_len, _ => Set::new())
  let adj_lists : Array[Array[Sort]] = Array::makei(sort_len, _ => Array::new()) // adj_lists[from] = [to0, to1, ...]
  for sort in 0..<sort_len {
    let rules : Array[Sentence] = grammar.rules[sort]
    for sentence in rules {
      for symbol in sentence {
        if sort_len <= symbol { // symbol is a Token
          first[sort].add(symbol)
          break // the following symbols will not appear in first!
        } else { // symbol is a Sort
          adj_lists[symbol].push(sort)
          if empty[symbol] == false {
            break // the following symbols will not appear in first!
          }
        }
      }
    }
  }

  // Process SCCs in topological order to propagate FIRST sets.
  // Within each SCC, all symbols have the same FIRST set due to mutual dependencies.
  // Then propagate FIRST sets from current SCC to dependent SCCs.
  let supergraph = SuperGraph::new(Graph::{ adj_lists, })
  let first_of_sccs = FixedArray::makei(supergraph.sccs.length(), _ => Set::new())
  for scc_i in supergraph.topology {
    for symbol in supergraph.sccs[scc_i].component { // union all first in the scc
      for f in first[symbol] {
        first_of_sccs[scc_i].add(f)
      }
      first[symbol] = first_of_sccs[scc_i]
    }
    for next_i in supergraph.scc_adj_lists[scc_i] { // scc adjacent
      for f in first_of_sccs[scc_i] {
        first_of_sccs[next_i].add(f)
      }
    }
  }
  (supergraph, first)
}

///|
fn LR1Maker::first_of(self : Self, sentence : Iter[Symbol]) -> Set[Token] {
  let first = Set::new()
  for symbol in sentence {
    if self.is_token(symbol) { // symbol is a Token
      first.add(symbol)
      break
    } else { // symbol is a Sort
      for f in self.first[symbol] {
        first.add(f)
      }
      if self.empty[symbol] == false {
        break // no longer to search the next symbol
      }
    }
  }
  first
}

///|
/// internally mutate state, and return it for convenience
fn LR1Maker::enclosure(self : Self, state : State) -> State {
  let worklist = state.items.0.to_array()
  while worklist.length() > 0 {
    let item = worklist.unsafe_pop()
    // item = A -> done ● sort tail | lookahead
    let cur = item.cur
    let sentence = item.sentence
    guard cur < sentence.length() else { continue }
    let sort = sentence[cur]
    guard false == self.is_token(sort) else { continue }
    let tail = item.sentence.iter().drop(cur + 1)
    // new_lookahead \in First(tail::lookahead)

    let new_lookaheads = self.first_of(
      tail.add(Iter::singleton(item.lookahead)),
    )
    let rules_of_sort = self.grammar.rules[sort]
    let len = rules_of_sort.length()
    for rule_i in 0..<len {
      for lookahead in new_lookaheads {
        let sentence : Sentence = rules_of_sort[rule_i]
        let new_item = Item::{ sort, rule_i, sentence, cur: 0, lookahead }
        if true == state.items.0.add_and_check(new_item) { // newly added
          worklist.push(new_item)
        }
      }
    }
  }
  state
}

///|
fn LR1Maker::add_state(self : Self, new_state : State) -> StateIndex {
  let new_state = self.enclosure(new_state)
  let new_index = self.goto_table.length()
  let mut actual_index = new_index
  self.state_indices.update(new_state, index => match index {
    None => // newly added
      Some(new_index)
    Some(index) => {
      actual_index = index
      Some(index)
    }
  })
  if actual_index == new_index { // newly added
    self.worklist.push((new_state, new_index))
    self.goto_table.push(Map::new())
    self.shift_table.push(Map::new())
    let reduce_map = Map::new()
    let first_set = Set::new()
    for item in new_state.items.0 {
      if item.cur < item.sentence.length() {
        let symbol = item.sentence[item.cur]
        if self.is_token(symbol) {
          first_set.add(symbol)
        } else { // symbol is a Sort
          for f in self.first[symbol] {
            first_set.add(f)
          }
        }
      } else { // item.cur == item.sentence.length()
        let lookahead = item.lookahead
        if false == reduce_map.contains(lookahead) {
          reduce_map[lookahead] = Set::new()
        }
        reduce_map[lookahead].add((item.sort, item.rule_i))
        first_set.add(lookahead)
      }
    }
    // assert reduce_map[lookahead].length() > 0 when lookahead \in reduce_map
    self.reduce_table.push(
      reduce_map
      .iter()
      .map(token_reduce_rules => {
        let (token, reduce_rules) = token_reduce_rules
        (token, reduce_rules.to_array())
      })
      |> Map::from_iter,
    )
    self.first_table.push(first_set)
  }
  actual_index
}

///|
pub fn LR1Maker::construct_states(self : Self) -> Unit {
  let initial_state = State::{
    items: ItemSet(
      Set::from_iter(
        Iter::singleton(Item::{
          sort: 0, // %START -> ● START | EOF
          rule_i: 0,
          sentence: [1],
          cur: 0,
          lookahead: self.sort_len,
        }),
      ),
    ),
  }
  let _ = self.add_state(initial_state)
  while self.worklist.length() > 0 {
    let (state, state_i) = self.worklist.unsafe_pop()
    let items = state.items.0
    // next_sort_rules[next] = [(sort, rule_i), ...] when next is a Token
    let next_sort_rules : Map[Symbol, Set[(Sort, RuleIndex)]] = Map::new()
    for item in items {
      let sentence : Sentence = item.sentence
      let cur = item.cur
      guard cur < sentence.length() else { continue }
      let next = sentence[cur]
      if false == next_sort_rules.contains(next) {
        next_sort_rules[next] = Set::new()
      }
      if self.is_token(next) {
        next_sort_rules[next].add((item.sort, item.rule_i))
      }
    }
    let new_states : Map[Symbol, State] = Map::from_iter(
      next_sort_rules
      .keys()
      .map(next => (next, State::{ items: ItemSet(Set::new()) })),
    )
    for item in items {
      let sentence : Sentence = item.sentence
      let cur = item.cur
      guard cur < sentence.length() else { continue }
      let next : Symbol = sentence[cur]
      new_states[next].items.0.add(Item::{ ..item, cur: cur + 1 })
    }
    for nn in new_states {
      let (next, new_state) = nn
      let state_j = self.add_state(new_state)
      if self.is_token(next) { // next is a Token
        self.shift_table[state_i][next] = (
          state_j,
          next_sort_rules[next].to_array(),
        )
        // assert shift_table[state_i][next].length() > 0 when next \in shift_table[state_i]
      } else {
        self.goto_table[state_i][next] = state_j
      }
    }
  }
  let states : Array[Set[Item]] = Array::make(
    self.state_indices.size(),
    Set::new(capacity=0),
  )
  for state_index in self.state_indices {
    let (state, index) = state_index
    states[index] = state.items.0
  }
  self.states = states
}

///|
pub fn LR1Maker::to_lr1(self : Self) -> LR1 {
  let { grammar, sort_len, states, .. } = self
  let state_len = states.length()
  let goto_table = Array::makei(self.sort_len, _ => Map::new(capacity=state_len))
  for state_i in 0..<state_len {
    for sort_next in self.goto_table[state_i] {
      let (sort, next) = sort_next
      goto_table[sort][state_i] = next
    }
  }
  let dfa_indices : Map[@dfa.TokenSet, Index] = Map::new()
  let dfa_tokens : Array[Array[Token]] = Array::new()
  fn add_dfa(tokens : Set[Token]) -> Index {
    let tokens = @dfa.TokenSet(tokens)
    let len = dfa_indices.size()
    match dfa_indices.get(tokens) {
      Some(index) => return index
      None => ()
    }
    dfa_indices.set(tokens, len)
    dfa_tokens.push(
      tokens.0.iter().map(token => token - self.sort_len).to_array(),
    )
    len
  }

  add_dfa(self.first_table[0]) |> ignore // ensure dfa_j of states[0] = 0
  let shift_table = self.shift_table
    .iter()
    .map(shift_map => shift_map.map((_token, next_sort_rules) => {
      let (next, _sort_rules) = next_sort_rules
      // when shifting, need to read next token from dfa
      (next, add_dfa(self.first_table[next]))
    }))
    .to_array()
  let reduce_table = self.reduce_table
    .iter()
    .map(reduce_map => reduce_map.map((_token, sort_rules) => {
      let (sort, rule_i) = sort_rules.iter().peek().unwrap()
      let rule_size = self.grammar.rules[sort][rule_i].length()
      (sort, rule_i, rule_size)
    }))
    .to_array()
  LR1::{
    grammar,
    sort_len,
    states,
    goto_table,
    shift_table,
    reduce_table,
    dfa_tokens,
  }
}
